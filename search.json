[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "bReaus",
    "section": "",
    "text": "POTW - Global mapping of river sediment bars\n\n\nPaper of the week by Carbonneau & Bizzi, 2024\n\n\n\npotw\n\n\ngeomorphology\n\n\nremote sensing\n\n\ndeep learning\n\n\nGoogle Earth Engine\n\n\nsediment\n\n\n\n\n\n\n\n\n\nJul 30, 2024\n\n\nLeo\n\n\n\n\n\n\n\n\n\n\n\n\nUsing data from OpenStreetMap in R\n\n\n\n\n\n\ncode\n\n\nr\n\n\nmaps\n\n\n\n\n\n\n\n\n\nJul 26, 2024\n\n\nRuben\n\n\n\n\n\n\n\n\n\n\n\n\nHosting a self updating Quarto dashboard on GitHub\n\n\n\n\n\n\ncode\n\n\nr\n\n\nquarto\n\n\ngithub\n\n\n\n\n\n\n\n\n\nJun 18, 2024\n\n\nRuben\n\n\n\n\n\n\n\n\n\n\n\n\nDeveloping packages in RStudio\n\n\n\n\n\n\ncode\n\n\nr\n\n\npackages\n\n\nrstudio\n\n\n\n\n\n\n\n\n\nJun 18, 2024\n\n\nLeo\n\n\n\n\n\n\n\n\n\n\n\n\nInstalling packages for ‘renv’\n\n\n\n\n\n\ncode\n\n\nr\n\n\nquarto\n\n\ngithub\n\n\n\n\n\n\n\n\n\nJun 12, 2024\n\n\nRuben\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/potw_240726_carbonneau24_globalSedimentBars/index.html",
    "href": "posts/potw_240726_carbonneau24_globalSedimentBars/index.html",
    "title": "POTW - Global mapping of river sediment bars",
    "section": "",
    "text": "How are sediment bars and banks distributed in our rivers and how does their area change over time and space? This question may not be widely considered, yet it is a topic worthy of further investigation. Most intact river systems are composed of a highly dynamic landscape within which sediment bars are key features. They are highly versatile with regard to water access and temperature, providing rich and diverse habitats and space for interactions among aquatic, avian, and terrestrial species, as well as important pathways of material exchange between terrestrial and aquatic systems (Hauer et al. 2016). A reduction in the extent of these features may indicate a atleration of the river system. For instance, the construction of dams and the implementation of channelisation measures can result in a constant inundation or plant overgrowth of fluvial sediments, thereby leading to a river landscape that is more stabilised and less habitat-rich. It is therefore crucial to gain a comprehensive understanding of the geographical distribution of these sedimentary features and to monitor their changes over time and across space.\nIn the new category “Paper of the Week”, I would like to introduce today the work of Patrice Carbonneau and Simone Bizzi. In their paper published this year (2024), they present an extraordinary analysis workflow applying state-of-the-art deep learning algorithms on satellite imagery to investigate the distribution of gravel bars in the global river network. Despite the technical nature of the work due to its promising methodological advancements, the results are just as interesting.\n\n\n\nSediment bar at the Vieux Rhône in France (own picture)\n\n\nIn terms of methodology, they utilised satellite imagery of an extent of 89 % of the worlds surface, sourced from the Sentinel-2 Satellite mission with a resolution of 10 meters. downloaded from Google Earth Engine and aggregated over one month (July 2021). Obtained from Google Earth Engine, the data was aggregated over a period of one month (July 2021). A fully convolutional neural network (FCNN) called Tiramisu, developed by Jégou et al. (2017), was applied to the dataset. This type of deep learning architecture is capable of pixel-level semantic class predictions. Each pixel was classified by the model with the following classifications:\n\nnoise ( background)\nriver\nlakes\nfluvial sediment deposits\nocean\nglaciers\nsnow\nclouds\n\nThe model was trained in two steps, first with a large dataset of automatic classifications from ESRI 2020 land-cover data, and then in a second step with manually classified images.\nTheir results show the relative area of rivers in the world (River and Stream Surface Area - RSSA), highlighting the regions with large shares of river surface area in Siberia and Alaska or in South-East Asia, but also in major basins such as the Amazon, Congo, Mississippi, Brahmaputra-Ganges and the Nile. By contrast, very few river area covered by rivers can be found in North and South-west Africa, the Middle East, Central- and West-Asia, as well as Australia.\n\n\n\nGlobal map of River and Stream Surface Area (RSSA) expressed as parts per million of surface area (104 ppm = 1%) for each 2° x 2° area (Carbonneau & Bizzi, 2024)\n\n\nThe Exposed Sediment Bar Surface Area (ESBSA) coincides mostly with the RSSA. Regions with the highest density of sediment bars are Siberia and Northern Russia with sediment produced in the Siberian uplands, Alaska with sediment sourced in the Brooks and Alaska mountain ranges, and the Nunavut region in Canada which is likely sourced in the Arctic cordillera mountains. In South America, the south-western foothills of the Andes show highest densities, transporting sediment to the Pacific Ocean. In Africa, highest gravel-densities can be found around the Licungo and Zambezi rivers debouching into the Mozambique channel, opposed by high sediment density rivers in Madagascar. Last, in Asia the highest concentrations of gravels can be found in rivers draining the Himalayas.\n\n\n\nExposed Sediment Bar Surface Area (ESBSA) expressed as parts per million of surface area (104 ppm = 1%) for each 2° x 2° area (Carbonneau & Bizzi, 2024)\n\n\nIn conclusion, Carbonneau and Bizzi present not only the first global study to quantify the area of sediment bars, but furthermore they are the first to apply FCNN on a nearly-global extent of satellite imagery to identify river systems and their key features and distinguish them from standing waters. The work showcases the potential of deep learning applied to global scale studies of fluvial systems, paving the way for future studies. As the analysis is conductible within 4 weeks of computation time, it could be used to constantly measure the location and concentration of sediments bars in the global river network each month, which would provide valuable insights into its change over time and space.\n\nLinks\nYou can find the whole article here:\nCarbonneau, Patrice E., and Simone Bizzi. “Global Mapping of River Sediment Bars.” Earth Surface Processes and Landforms 49, no. 1 (2024): 15–23. https://doi.org/10.1002/esp.5739.\nIf you want to read more about the ecological significance of sediment bars I recommend:\nHauer, F. Richard, Harvey Locke, Victoria J. Dreitz, Mark Hebblewhite, Winsor H. Lowe, Clint C. Muhlfeld, Cara R. Nelson, Michael F. Proctor, and Stewart B. Rood. “Gravel-Bed River Floodplains Are the Ecological Nexus of Glaciated Mountain Landscapes.” Science Advances 2, no. 6 (June 24, 2016): e1600026. https://doi.org/10.1126/sciadv.1600026.\nAnd if you want to learn more about the fully convolutional neural network Tiramisu, take a look at:\nJégou, Simon, Michal Drozdzal, David Vazquez, Adriana Romero, and Yoshua Bengio. “The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for Semantic Segmentation.” In 2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 1175–83, 2017. https://doi.org/10.1109/CVPRW.2017.156."
  },
  {
    "objectID": "posts/renv_install_packages/index.html",
    "href": "posts/renv_install_packages/index.html",
    "title": "Installing packages for ‘renv’",
    "section": "",
    "text": "renv is a R package that allows to create environments, where the user can install and manage packages. This is needed when using a GitHub action, when you want to render quarto document remote.\nTo use renv it is necessary to install R packages again, even though they are already installed locally. This enables to use GitHub actions to automatically render the Quarto-Project to deploy a GitHub page!\nPackages can be installed normally with install.packages('tidyverse'). Afterwards you can check the status of renv with renv::status(). You can also install all necessary packages (e.g. knitr, markdown,…) by renv::install(). If you installed new packages you have to run renv::snapshot(), to add the installed packages to the lockfile. The lockfile holds all the information about the used packages and when the build and deploy action is performed all the packages will be installed.\n\n\n\nOutput of the build-deploy action on GitHub\n\n\nFor more information on how to use GitHub-actions with Quarto click here!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "bReaus\nR, bros and eau!"
  },
  {
    "objectID": "posts/dashboard_self_update/index.html",
    "href": "posts/dashboard_self_update/index.html",
    "title": "Hosting a self updating Quarto dashboard on GitHub",
    "section": "",
    "text": "Dashboards are cool and self updating dashboards are even cooler! This is short insight how to create something like this.\n\n\nIntroduction\nThe idea of having interactivity and updating content on a static website somehow fascinates me! So I tried to use rvest to scrape a german website which hosts relative up-to-date discharge values of rivers in Baden-Würrtemberg. Sadly I was not able to read the table (yet!), but I tried something else.\nThe German Weather Service (DWD) has a R-package called rdwd. It is a great tool to access a lot of different climate data in Germany and the world! So the idea was to build a dashboard to present the recent climate in Freiburg and add more features in the near future.\n\n\nWhat is a Quarto dashboard?\nA Quarto dashboard is a website consisting of one or more pages, where you can visualize your data, do some monitoring and also add interactivity (Shiny and ojs)! To create one yourself, you just have to create a new Quarto project with a .qmd file and change the format to dashboard. Make sure to enable renv and git.\n\nNote: the output file is called index.html, so the website can be hosted later over GitHub pages.\n\n---\ntitle: \"Das Klima in Freiburg\"\nformat: \n  dashboard: \n    output-file: index.html\n---\n\n\nFill the dashboard \nThe first thing to do, is to get the data from the DWD API. Therefore we load the package rdwd and also bit64, which is required. With selectDWD() you can select the location of your station, the variables you want to download and the time resolution. In my case I selected Freiburg, air temperature and precipitation in an hourly resolution. There are many more variables to download, an exact tutorial can be found in the documentation.\n# Load required packages\nrequire(bit64)\nrequire(rdwd)\nrequire(lubridate)\nrequire(plotly)\nrequire(dplyr)\nrequire(tidyr)\nrequire(ggplot2)\n\n\n# get link to DWD station in Freiburg (for temperature and precipitation, hourly)\nlink &lt;- selectDWD(\"Freiburg\", res=\"hourly\", var=c(\"air_temperature\", \"precipitation\"), per=\"recent\")\n\n# download and read DWD data\nfile &lt;- dataDWD(link, read = T, varnames = T)\n\n# get current time and data and make sure tz='Berlin/Germany'\ncurrTime &lt;- as.POSIXct(Sys.time()) + 2*60*60 \nYou can create the layout of the dashboard using ## Row or ## Column in your document. Here I added some value boxes to display the date and time of the last update of the dashboard. For the icon you can use all the Bootstrap Icons. To change the color check out the documentation.\n#| content: valuebox\n#| title: \"Datum\"\nlist(\n  icon = \"calendar-check\",\n  color = \"light\",\n  value = as.Date(currTime)\n)\n#| content: valuebox\n#| title: \"Uhrzeit des letzten Updates\"\nlist(\n  icon = \"clock\",\n  color = \"light\",\n  value = strftime(currTime, format=\"%H:%M\")\n)\nNow we just need to wrangle our data a little and visualize it. I created a ggplot with a facet_wrap() and used ggplotly() to give the graph some interactivity.\n# make a tibble of the downloaded data and tidy up!\ndf &lt;- file$hourly_air_temperature_recent_stundenwerte_TU_01443_akt |&gt; \n  left_join(file$hourly_precipitation_recent_stundenwerte_RR_01443_akt, by = \"MESS_DATUM\") |&gt; \n  select(-c(eor.y,QN_8,QN_9,eor.x,STATIONS_ID.y)) |&gt; \n  pivot_longer(cols = -c(STATIONS_ID.x, MESS_DATUM))\n\n#wanted time period to display the data (in this case 5 days)\ntime_period &lt;- c(as.Date(currTime) - 5, as.Date(currTime))\n\n# Filter the data and rename variables\ndf_plot &lt;- df |&gt; \n  filter(MESS_DATUM &gt;= time_period[1]) |&gt; \n  mutate(name = case_when(\n    name == 'TT_TU.Lufttemperatur' ~ 'Lufttemperatur (°C)',\n    name == 'RF_TU.Relative_Feuchte' ~ 'Relative Feuchte (%)',\n    name == 'R1.Niederschlagshoehe' ~ 'Niederschlagshöhe (mm)',\n    name == 'RS_IND.Niederschlagsindikator' ~ 'Niederschlagsindikator',\n    name == 'WRTR.Niederschlagsform' ~ 'Niederschlagsform'\n  )) |&gt; \n  filter(name == 'Lufttemperatur (°C)' | \n           name == 'Relative Feuchte (%)' |\n           name == 'Niederschlagshöhe (mm)') |&gt; \n  rename(Datum = MESS_DATUM,\n         Wert = value)\n\n# plotting \nggplotly(\n  ggplot(data = df_plot) + \n    geom_line(aes(x = Datum, y = Wert)) + \n    facet_wrap(~name, scales = 'free_y') +\n    theme_bw()\n)\nHey! Now we have a (very simple) dashboard! But we have to render it ourself to get the newest values of the DWD database and can only use it locally. This is where we have to…\n\n\nUse GitHub-actions and -pages to publish the dashboard \nThis next step is actually quite easy. You have to create a new repository on GitHub, init, add remote, add ., commit and push (the usual stuff!) and add a .github folder in your Quarto project. In there you create a folder called workflows and in which you create a file: publish.yml. You can find more information here.\n\n\n.github/workflows/publish.yml\n\non:\n  workflow_dispatch:\n  push:\n    branches: main\n  schedule:\n    - cron: '0 */2 * * *'\n\nname: Quarto Publish\n\njobs:\n  build-deploy:\n    runs-on: ubuntu-latest\n    permissions:\n      contents: write\n    steps:\n      - name: Check out repository\n        uses: actions/checkout@v4\n\n      - name: Set up Quarto\n        uses: quarto-dev/quarto-actions/setup@v2\n\n      - name: Install R\n        uses: r-lib/actions/setup-r@v2\n        with:\n          r-version: '4.2.0'\n\n      - name: Install R Dependencies\n        uses: r-lib/actions/setup-renv@v2\n        with:\n          cache-version: 1\n\n      - name: Render and Publish\n        uses: quarto-dev/quarto-actions/publish@v2\n        with:\n          target: gh-pages\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n\nThe magic happens through the schedule, where you can set a cron expression when to render and publish your dashboard. I have the feeling this is a science to itself so here is a webiste to create the cron expression you desire. In my case I added a cron expression to run the action every two hours. In my time output I realized that there is a time difference of 2 hours from the GitHub servers to central european time, have that in mind!\non:\n  workflow_dispatch:\n  push:\n    branches: main\n  schedule:\n    - cron: '0 */2 * * *'\nThe last thing to do now is to add a branch called gh-pages to your repository and push the publish.yml. Activate Github pages under the setting of your repository and choose gh-pages as the branch to deploy from. Now every two hours and with every push my dashboard will be updated!\n\n\nWhat now? \nThese were really the basics to start a dashboard, but for me there will be some more steps to make the data visualization and information better. You can see the live dashboard under https://carluna.github.io/klima-freiburg/ and view the whole code here: https://github.com/Carluna/klima-freiburg!"
  },
  {
    "objectID": "posts/package_development_rstudio/index.html",
    "href": "posts/package_development_rstudio/index.html",
    "title": "Developing packages in RStudio",
    "section": "",
    "text": "Why to write a package? Packages provide an easy way to share code with others or use it later, saving time by organizing the code and projects in standardized way.\nPhilosophy: anything that can be automated, should be automated.\nPrerequisites: install.packages(c(\"devtools\", \"roxygen2\", \"testthat\", \"knitr\"))"
  },
  {
    "objectID": "posts/package_development_rstudio/index.html#intro",
    "href": "posts/package_development_rstudio/index.html#intro",
    "title": "Developing packages in RStudio",
    "section": "",
    "text": "Why to write a package? Packages provide an easy way to share code with others or use it later, saving time by organizing the code and projects in standardized way.\nPhilosophy: anything that can be automated, should be automated.\nPrerequisites: install.packages(c(\"devtools\", \"roxygen2\", \"testthat\", \"knitr\"))"
  },
  {
    "objectID": "posts/package_development_rstudio/index.html#basic-workflow",
    "href": "posts/package_development_rstudio/index.html#basic-workflow",
    "title": "Developing packages in RStudio",
    "section": "Basic workflow",
    "text": "Basic workflow\n\ncreate package either via devtools::create(path) or via RStudio–&gt;new Project–&gt;new R package\nin case git should be used: devtools::use_git()\nediting functions:\n\nusethis::use_r(\"functionname\") to create a new function script, naming the file after the function. New .R-files for each user-facing function in the package. After adding more functions, these might be grouped.\ncreate roxygen2-skeleton to provide information and document functions via 'Ctrl + Alt + Shift + R'\ndevtools::load_all() to load the created functions under R/ subdirectory\nusethis::use_package() to include functions from other packages (adding them to the Imports field of DESCRIPTION). The function can be used via packagename::fun()\n(git-commit after editing a function)\ndevtools::rename_files(\"old_name\", \"new_name\") to update the function name in files –&gt; Don’t forget to update test-files, too!\n\ncheck that an R package is in full working order with devtools::check() or 'Ctrl + Shift + E', providing a convenient way to run this without leaving the R session\ncreate testing infrastructure via usethis::use_testthat()\n\nusethis::use_test(\"function_name\") to create a test-file for a specific function\ndevtools::test() or 'Ctrl + Shift + T' to run test\n\npick license, e.g. via usethis::use_mit_license(), creating LICENSE and LICENSE.md-files\ncreate and update documentation via devtools::document()\nuse_readme_rmd() initializes a basic, executable README.Rmd file for the github page to describe the purpose of the package, providing installation instructions, and showing a bit of usage. build_readme() renders the file\nInstallation: devtools::check() again, then install package into library via devtools::install()\nPublish: devtools::build()converts package folder/project into single bundled file"
  },
  {
    "objectID": "posts/package_development_rstudio/index.html#structure",
    "href": "posts/package_development_rstudio/index.html#structure",
    "title": "Developing packages in RStudio",
    "section": "Structure",
    "text": "Structure\n\n\n\n\n\n\n\nRecommended folder structure for packages (Posit Software, 2024)\n\n\n\nDESCRIPTION provides metadata about the package\nNAMESPACE declares the functions the package exports for external use and the external functions your package imports from other packages. is automatically edited when using roxygen2\n.Rbuildignore lists files that are needed to be around but should not be included when building the R package from source\n.Rproj.user - directory used internally by RStudio\n.gitignore anticipates Git usage and tells Git to ignore some standard, behind-the-scenes files created by R and RStudio\ntests/ directory in which the testing framework is placed, containing specific tests for the functions"
  },
  {
    "objectID": "posts/package_development_rstudio/index.html#main-functions-during-development",
    "href": "posts/package_development_rstudio/index.html#main-functions-during-development",
    "title": "Developing packages in RStudio",
    "section": "Main functions during development",
    "text": "Main functions during development\nThese functions setup parts of the package and are typically called once per package:\n\ncreate_package()\nuse_git()\nuse_mit_license()\nuse_testthat()\nuse_github()\nuse_readme_rmd()\n\nThese functions are called on a regular basis, as adding functions and tests or taking on dependencies:\n\nuse_r()\nuse_test()\nuse_package()\n\nThese functions are called multiple times per day or per hour, during development:\n\nload_all()\ndocument()\ntest()\ncheck()"
  },
  {
    "objectID": "posts/package_development_rstudio/index.html#main-shortcuts",
    "href": "posts/package_development_rstudio/index.html#main-shortcuts",
    "title": "Developing packages in RStudio",
    "section": "Main shortcuts",
    "text": "Main shortcuts\n\nCtrl + Shift + T - run devtools::test()\nCtrl + Shift + E - run devtools::check()\nCtrl + Shift + Alt + R - create roxygen2 skeleton"
  },
  {
    "objectID": "posts/package_development_rstudio/index.html#package-states",
    "href": "posts/package_development_rstudio/index.html#package-states",
    "title": "Developing packages in RStudio",
    "section": "Package states",
    "text": "Package states\n\nsource - directory of files with specific package structure\nbundled - compressed into single file using extension .tar.gz\nbinary - single platform-specific file\ninstalled - binary package that’s been compressed into a package library\nin-memory - package loaded into memory\n\n\n\n\nMethods for converting between package states (Wickham & Bryan 2023)"
  },
  {
    "objectID": "posts/package_development_rstudio/index.html#more-info",
    "href": "posts/package_development_rstudio/index.html#more-info",
    "title": "Developing packages in RStudio",
    "section": "more info",
    "text": "more info\nWriting R Extensions (r-project.org)\nR packages book (2e)"
  },
  {
    "objectID": "posts/osm_in_R/index.html",
    "href": "posts/osm_in_R/index.html",
    "title": "Using data from OpenStreetMap in R",
    "section": "",
    "text": "The OpenStreetMap community provides free and open geographic data! You may wonder how to use this data in R and that is what we’ll cover in this post.\n\nWhich packages? \nWe’ll use the package osmdata to get access to the OpenStreetMap (OSM) data. sf gives us the basic functions to work with geographic data and we use dplyr and tidyr for data manipulation. ggplot2 and geomtextpath will be used for the visualization.\n\nrequire(sf)\nrequire(osmdata)\nrequire(dplyr)\nrequire(tidyr)\nrequire(ggplot2)\nrequire(geomtextpath)\n\n\n\nLoad the data \nFirst we have to set the extent of which we want to get the data. I chose the district La Guillotiere-Sud in Lyon.\n\n# Create bounding box of desired extent\nextent &lt;- tibble(\n  lon = c(4.833075520540814, 4.851088606828741),\n  lat = c(45.747993822856955, 45.755631222240936)  \n)\n\nbbox &lt;- extent %&gt;% \n  st_as_sf(coords = c(\"lon\", \"lat\"), crs = \"WGS84\") %&gt;% \n  st_bbox() \n\nNext we will use the function opq to build a query to the OSM database with the information of our extent bbox. With add_osm_feature() we can download different features from OSM, in our case these are “building”, “highway” and “waterway”. Here you can see the full list of possibilities. osmdata_sf will return the desired feature as an sf object. We will intersect the output again because the extent of the received data is slightly bigger.\n\n# Get polygons of the feature building\ndf_build &lt;- opq(bbox = bbox) |&gt; \n    add_osm_feature(key = \"building\", value_exact = FALSE) |&gt; \n    osmdata_sf()  \n\ndf_build_poly &lt;- df_build$osm_polygons |&gt; \n  select(geometry, building, public_transport) # get all buildings and information about usage as public transport\n\n# Get lines of the feature highway\ndf_highw &lt;- opq(bbox = bbox) |&gt; \n    add_osm_feature(key = \"highway\", value_exact = FALSE) |&gt; \n    osmdata_sf()  \n\ndf_streets &lt;- df_highw$osm_lines |&gt; \n  select(geometry, name) |&gt; \n  st_intersection(st_as_sfc(bbox)) |&gt; \n  na.omit() |&gt; # get rid of small features with no names\n  group_by(name) |&gt; \n  # Here we summarise the geometries of the streets, this is necessary to plot the names of the streets later\n  summarise(geometry = st_union(geometry)) |&gt; \n  ungroup()\n\n# Get lines of the feature waterway\ndf_waterw &lt;- opq(bbox = bbox) |&gt; \n    add_osm_feature(key = \"waterway\", value_exact = FALSE) |&gt; \n    osmdata_sf() \n\ndf_river &lt;- df_waterw$osm_lines |&gt; \n  select(geometry, name)|&gt; \n  st_intersection(st_as_sfc(bbox))\n\n\nNote: We actually do not only get the polygons or lines but also the features which are points or other geometries.\n\n\nnames(df_build)\n\n[1] \"bbox\"              \"overpass_call\"     \"meta\"             \n[4] \"osm_points\"        \"osm_lines\"         \"osm_polygons\"     \n[7] \"osm_multilines\"    \"osm_multipolygons\"\n\n\n\n\nPlot the data \nNow we just have to plot our data. I use the function geom_textsf() from the package geomtextpath to add labels to the river and the streets. Note that you have to declare that you used the data from OSM, see here for more informations. In the case of buildings, we differentiate between whether they are public transportation or not. This is because the subway stations are also part of buildings. We put them in a lighter gray under the streets!\n\nggplot() +\n  geom_textsf(data = df_river,\n          aes(geometry = geometry,\n              label = name),\n          text_smoothing = 99.5,\n          linecolour = \"#8888B3\",\n          linewidth = 20,\n          color = \"black\", \n          alpha = 0.8, \n          size = 3, \n          hjust = 0.01,\n          vjust = -0.01) +\n  geom_sf(data = df_build_poly |&gt; filter(!is.na(public_transport)),\n          aes(geometry = geometry),\n          fill = \"grey90\") +\n  geom_textsf(data = df_streets,\n          aes(geometry = geometry,\n              label = name),\n          color = \"black\",\n          linecolor = \"black\",\n          alpha = 1,\n          fontface = 3,\n          size = 1.5,\n          remove_long = T) +\n  geom_sf(data = df_build_poly |&gt; filter(is.na(public_transport)),\n          aes(geometry = geometry),\n          fill = \"grey50\") +\n  theme_void() +\n  labs(title = \"LYON - LA GUILLOTIÈRE-SUD\",\n       caption = \"Data from OpenStreetMap\") +\n  theme(plot.title = element_text(face = \"bold\"),\n        plot.caption = element_text(size = 7))\n\n\n\n\n\n\n\n\nTo be honest, the street names are not really readable and a bit confusing in this map. So lets do it again without!\n\nggplot() +\n  geom_textsf(data = df_river,\n          aes(geometry = geometry,\n              label = name),\n          text_smoothing = 99.5,\n          linecolour = \"#8888B3\",\n          linewidth = 20,\n          color = \"black\", \n          alpha = 0.8, \n          size = 3, \n          hjust = 0.01,\n          vjust = -0.01) +\n  geom_sf(data = df_build_poly |&gt; filter(!is.na(public_transport)),\n          aes(geometry = geometry),\n          fill = \"grey90\") +\n  geom_sf(data = df_streets,\n          aes(geometry = geometry),\n          color = \"black\",\n          linewidth = 0.8\n          ) +\n  geom_sf(data = df_build_poly |&gt; filter(is.na(public_transport)),\n          aes(geometry = geometry),\n          fill = \"grey50\") +\n  theme_void() +\n  labs(title = \"LYON - LA GUILLOTIÈRE-SUD\",\n       caption = \"Data from OpenStreetMap\") +\n  theme(plot.title = element_text(face = \"bold\"),\n        plot.caption = element_text(size = 7))\n\n\n\n\n\n\n\n\n\n\nFurther ressources\n\nThe package osmdata: https://github.com/ropensci/osmdata\nThe package osmectract: https://github.com/ropensci/osmextract/\nWebsite of OSM: https://www.openstreetmap.org/"
  }
]